{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, hamming_loss\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from mymodel import make_model, extract_features_labels, predict\n",
    "\n",
    "## function to remove outliers from data, you can add more filters here!\n",
    "def trip_invalid(trip):\n",
    "    ## remove trips that dont end at (-90) +- 15 degrees\n",
    "    if (trip[-1,17].astype(float) > (-90)+15) or (trip[-1,17].astype(float) < (-90)-15) :\n",
    "        return True\n",
    "    \n",
    "    ## remove trips that enter right lane too early\n",
    "    for j, t in enumerate(trip):\n",
    "        if (t[0].astype(float)>0) and (t[1].astype(float) < -20):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "## function to get frequency of each unique label in the dataset\n",
    "def action_count(labels):\n",
    "    freq = {}\n",
    "    for label in labels:\n",
    "        if label not in freq.keys():\n",
    "            freq[label] = 1\n",
    "        else:\n",
    "            freq[label] += 1\n",
    "    return freq\n",
    "\n",
    "## function to do preprocessing of data, like data pruning, conversion to float, manage action labels, etc\n",
    "def preprocess_raw(data_path = \"dataset_basic_103_trips.npy\"):\n",
    "    trips = np.load(data_path)\n",
    "    ## Now, apply a window over labels, if left/right is taken in vicinity of a frame, that'll be its label\n",
    "    ## This helps us to remove the bias towards no action and also gives us a better sense ...\n",
    "    ## of how the car behaved close to those frames rather than at that exact frame\n",
    "    win = [-4, 4]\n",
    "    wina = [-2, 2]\n",
    "    dataset = []\n",
    "    for i, trip in enumerate(trips):\n",
    "        ## call function to prune data, eg. remove trip that doesn't end at (-90)+-15 degrees\n",
    "        if trip_invalid(trip):\n",
    "            continue\n",
    "        for j, t in enumerate(trip):\n",
    "            ## convert string label to integer\n",
    "            t[-1] = action[t[-1]]   # the change is automatically reflected back in trip\n",
    "            \n",
    "            count = 0\n",
    "            for k in range(win[0], win[1]):\n",
    "                if j+k > 0 and j+k < len(trip) and trip[j+k,-1] == 3:    ## RIGHT\n",
    "                    count = count + 1\n",
    "                if j+k > 0 and j+k < len(trip) and trip[j+k,-1] == 2:    ## LEFT\n",
    "                    count = count - 1\n",
    "            if count > 0:\n",
    "                t[-1] = 3\n",
    "            elif count < 0:\n",
    "                t[-1] = 2\n",
    "\n",
    "            count = 0\n",
    "            for k in range(wina[0], wina[1]):\n",
    "                if j+k > 0 and j+k < len(trip) and trip[j+k,-1] == 0:    ## UP\n",
    "                    count = count + 1\n",
    "                elif j+k > 0 and j+k < len(trip) and trip[j+k,-1] == 1:    ## DOWN\n",
    "                    count = count - 1\n",
    "            if count > 0:\n",
    "                t[-1] = 0\n",
    "            elif count < 0:\n",
    "                t[-1] = 1   \n",
    "        dataset.append(trip)\n",
    "    print(\"Total number of trips before pruning: \", trips.shape[0])\n",
    "    print(\"Total number of trips after pruning: \", len(dataset))\n",
    "    dataset = np.concatenate(dataset)\n",
    "    print(\"dataset shape: \", dataset.shape)\n",
    "    \n",
    "    dataset = dataset.astype(float)\n",
    "    return dataset\n",
    "\n",
    "## function to perform min-max scaling over your data\n",
    "def min_max_scaling(traindata, testdata):\n",
    "    scale=1\n",
    "    min_xval = np.min(traindata,0)\n",
    "    max_xval = np.max(traindata,0)\n",
    "    traindata = (traindata - min_xval)/(max_xval - min_xval)\n",
    "    traindata = np.nan_to_num(traindata)\n",
    "    testdata = (testdata - min_xval)/(max_xval - min_xval)\n",
    "    testdata = np.nan_to_num(testdata)\n",
    "    return traindata, testdata, min_xval, max_xval\n",
    "\n",
    "## visualize orientation of car vs distance from (10,0) for all points across the dataset\n",
    "def orientation_vs_dist(x,y,o):\n",
    "    ref = [10,0]\n",
    "    d = np.square(x-ref[0])+np.square(y-ref[1])\n",
    "    plt.scatter(d,o,marker='.')\n",
    "    plt.xlabel(\"squared distance from (10,0)\")\n",
    "    plt.ylabel(\"orientation\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "## extract features according to your design here (feature modeling/engineering)\n",
    "def extract_features_labels(data, net=\"mlp_all_labels\"):\n",
    "    def py_ang(v1, v2):\n",
    "        cosang = np.dot(v1, v2)\n",
    "        sinang = la.norm(np.cross(v1, v2))\n",
    "        return np.arctan2(sinang, cosang)\n",
    "    \n",
    "    if net == \"mlp_all_labels\":\n",
    "        labels = data[:,22].reshape(-1, 1)\n",
    "        labels = one_hot_encoding(labels)\n",
    "\n",
    "        data = data.tolist()\n",
    "        features = []\n",
    "        for i, dat in enumerate(data):\n",
    "            ## {X,Y} location; {X,Y} velocity; Y dist of car from (10,10); orientation; {X,Y} acceleration\n",
    "            feat = [dat[0], dat[1], dat[2], dat[3], dat[4], dat[17], dat[18], dat[19]]\n",
    "\n",
    "            feat.append(10 - dat[0])    ## X distance from lower junction(10,-10)\n",
    "            feat.append(-10 - dat[1])    ##Y distance from lower junction\n",
    "            vec1 = [10 - dat[0], 10 - dat[0]]    ## line connecting (10,10) and car X \n",
    "            vec2 = [10 - dat[1], -10 - dat[1]]   ## line connecting (10,-10) and car X\n",
    "            ang = py_ang(vec1, vec2)*(180/math.pi)   ## angle formed by these lines\n",
    "            feat.append(ang)    ##(angle at which car approaches the junction)\n",
    "            if dat[0] > 0:    ## lane where the car is (0 for left and 1 for right)\n",
    "                feat.append(1)\n",
    "            else:\n",
    "                feat.append(0)\n",
    "            features.append(np.array(feat))\n",
    "        features = np.array(features)\n",
    "    elif net == \"orientation_regression\":\n",
    "#         labels = data[:,17].reshape(-1,1)/180\n",
    "        labels = data[:,17].reshape(-1,1)\n",
    "        \n",
    "        data = data.tolist()\n",
    "        features = []\n",
    "        ref = [10,0]\n",
    "        for i, dat in enumerate(data):\n",
    "            ## {X,Y} location; {X,Y} velocity; Y dist of car from (10,10); acceleration\n",
    "            feat = [dat[0], dat[1], dat[2], dat[3], dat[4], dat[18], dat[19]]\n",
    "\n",
    "            feat.append(10 - dat[0])    ## X distance from (10,10)\n",
    "\n",
    "            d = np.square(dat[0]-ref[0])+np.square(dat[1]-ref[1])    ## abs dist of car from (10,0)\n",
    "            feat.append(d)\n",
    "            \n",
    "            vec1 = [10 - dat[0], 10 - dat[0]]    ## line connecting (10,10) and car X \n",
    "            vec2 = [10 - dat[1], -10 - dat[1]]   ## line connecting (10,-10) and car X\n",
    "            ang = py_ang(vec1, vec2)*(180/math.pi)   ## angle formed by these lines\n",
    "            feat.append(ang)    ##(angle at which car approaches the junction)\n",
    "            \n",
    "            features.append(np.array(feat))\n",
    "        features = np.array(features)\n",
    "    else:\n",
    "        print(\"net not recognized\")\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def train(model, epoch, trainloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        #if torch.cuda.is_available():\n",
    "        #    data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), loss.data[0]))\n",
    "\n",
    "def get_accuracy(model, data, labels, net):\n",
    "    preds = predict(model, data)\n",
    "    if net == \"mlp_all_labels\":\n",
    "        print(\"r2 score: \", r2_score(Y_train,preds))\n",
    "        acc = r2_score(Y_train,preds,multioutput='raw_values')\n",
    "        print(\"r2 score for each label: \", acc)\n",
    "        t = len(labels)\n",
    "        print(\"total number of test elements: \", t)\n",
    "        print(\"#up in preds:\", np.count_nonzero(preds[:,0]),\n",
    "              \",#up in actual:\", np.count_nonzero(labels[:,0]),\n",
    "              \",acc:\",np.count_nonzero(preds[:,0]==labels[:,0])/t)\n",
    "        print(\"#down in preds:\", np.count_nonzero(preds[:,1]),\n",
    "              \",#down in actual:\", np.count_nonzero(labels[:,1]),\n",
    "              \",acc:\",np.count_nonzero(preds[:,1]==labels[:,1])/t)\n",
    "        print(\"#left in preds:\", np.count_nonzero(preds[:,2]),\n",
    "              \",#left in actual:\", np.count_nonzero(labels[:,2]),\n",
    "              \",acc:\",np.count_nonzero(preds[:,2]==labels[:,2])/t)\n",
    "        print(\"#right in preds:\", np.count_nonzero(preds[:,3]),\n",
    "              \"#right in actual:\", np.count_nonzero(labels[:,3]),\n",
    "              \",acc:\",np.count_nonzero(preds[:,3]==labels[:,3])/t)\n",
    "        print(\"#no_control in preds:\", np.count_nonzero(preds[:,4]),\n",
    "              \"#no_control in actual:\", np.count_nonzero(labels[:,4]),\n",
    "              \",acc:\",np.count_nonzero(preds[:,4]==labels[:,4])/t)\n",
    "    elif net == \"orientation_regression\":\n",
    "        acc = np.mean(np.abs(preds-labels))\n",
    "        print(\"average error in degree:\", acc)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total number of trips before pruning: ', 103)\n",
      "('Total number of trips after pruning: ', 71)\n",
      "('dataset shape: ', (9595, 23))\n",
      "('freq of each action after preprocessing: ', {0: 871, 1: 4, 3: 3023, 4: 5697})\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEKCAYAAAA8QgPpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt8XOV16P3f2jOjK/JNFpZ8kWWB7YDs1mCDTeoQaEmw\nHRKgbxoIOTklHErIIedt8ranhZDDy2mblDQnad40TYib0jYplCRNIDRgEiABJ+dgG9sYLEF8Qb5g\nWzKyLFmybqOZWe8fe894ZjQjj6UZzZa8vp/PfDR7z569n9myZ2k/az/rEVXFGGOMOVdOsRtgjDFm\ncrIAYowxZkwsgBhjjBkTCyDGGGPGxAKIMcaYMbEAYowxZkwsgBhjjBkTCyDGGGPGxAKIMcaYMQkW\nuwGFNHv2bG1oaCh2M4wxZlLZsWPHCVWtOdt2UzqANDQ0sH379mI3wxhjJhUROZTLdtaFZYwxZkws\ngBhjjBkTCyDGGGPGxAKIMcaYMbEAYowxZkwmXQARkXUiskdE9ovIvcVujzHGnK8m1W28IhIA/h54\nH3AEeEVEnlLVN4rbMn/bcaiLLa2dzKwo4clXj7C/o485VaUAHO0eQAQEOD0UJRJzZ6h0BGJJk1UK\nYHNXGjP5HHzoAwXb96QKIMCVwH5VbQUQkceBGwELIFnsONTFx76zhXAklhIQTvaFR31fLC1aWPAw\nZnJquPfpggWRydaFNQ94O2n5iLcuQUTuEpHtIrK9o6NjQhvnR1taO0cED2OMyYfJFkDOSlU3quoq\nVV1VU3PWkfhT3prGakqCDo4UuyXGmKlmsnVhHQUWJC3P99adV3Yc6uLHO4+w73gvLcd66A9HrYvJ\nGJOR5UDOeAVYLCKLcAPHrcBtxW3SxNpxqIuP/oOb0zCZZUr4BxxQTc3trJg/nV1HTiWWHSAYdBiO\nxDIG5JtWzOXJXceyLqfv7+rFs9l28CThSIySoMMDNzTxFz9tyXn50TvXACRyWBP5nlz28+ida9jT\n3svnntid+MxfvHk5t62uP4fflpnMJlUAUdWIiHwa+BkQAB5R1ZYiN2tCbWntZNiCx6gyfflHM5yy\n5mM9KcsxyBo8AF7c2zHqcvr+4sEjpu5+NzW3ndPyltZOgKK8J5f9bGntTGwXt6m5zQLIeWTS5UBU\n9RlVXaKqF6nqF4rdnom2prGaUHDS/domVKZ0T8BhRB5o2dxpKcsOEAo6Gd8PcM2SmlGX0/d3ZcMs\nSoIOAXH3u35Z3Tktr2msTuSwJvo9uexnTWM165fVpXzm9GUztU2qKxADz7W0UxZ0iERiTKXrEEcg\n6AjRmCICQcdBBKIxpaosSDiqlAYdQo7QcTqMCFw4rYyKoMOBzj4EIRgQls6pQnH/cu7sG2IgHOWS\numn8+fpLAPj0ozt4p3eI5fOm8+Sn1/KZx1/l+TePs2BmBX9183Kea2nn2ZZ26mdVoEB1ZQmdfWHW\nL6vjttX1XLmomk3NbVmXP/P4q7y4t4NrltTwtVsvS4zBWdNYzcqFM1laW3VOywCP3rmmKO/JZT/x\n7ZLPgTl/iOrUTb+uWrVKp9J8IA898yYPb24tdjPyYv6MMo50DyaWZ5QH6R6IJJZvWjGXr9162aj7\niI9xGRp2u53Scx+hgBuQ4v31z7W0p5y/m1bM5dmW9kSf/u1XNaS8bv355nwlIjtUddXZtrO+kEnk\n2Zb2Yjchb44mBQ8gJXjAyPxCJvExLvGgkf6n0HBUU/rr08/fi3s7Uvr001/f1NyW02cx5nxlAWQS\nWddUW+wm5M28GWUpyzPKU3tT0/MLmSTGuHjL6bmLUEBS+uvTz981S2pS+vTTX7f+fGNGZzmQSWDH\noS6+/dJb7DzcRUAgWsBex6ADsZh7R1LysQQ3T5G+rImfAqqJ22QdB4IiDEUVASpKA0QiMYZjym95\n+YebvvFrmo/1sGzutEQ+Ijl/EP/sP9p5BAF+//L57Gnv5ZFft4IId/zOIh69cw0/2nmEE71D1FSV\n0jcUYdfb3axrquV9TbV8+6W3ON4zyJ72Xu7d4OZBnm1pZ11TLfduuGREjqK+utL6843JkeVAfG7H\noS5u3fgyw4WMGuMQdNyxFSJuviEbceNLQq45jo9ufJmw99kDzshj3H11I//88kHCkRhBb7BHxMt7\nWE7DmLGxHMgUsaW107fBAyAScwPIaMEDUoMH5J7jSP7smY4RT4LH8xjJeQ/LaRhTWBZAfG5NYzWh\ngH8LWQUdt6srcJZiW5L2cq45juTPnukY65pqU/IYyXkPy2kYU1iWA5kErl16Ia8c7ORUfwTl3Eur\nOwLTyoIj7nTKpCLkICIMDEcT4zKiCqUBoaoixHAkRn84ioiwYZn7Bf38m8epn1VBScDhN8d7mV1Z\nQjDgUD+rgmPdA4l8xbYDnYkxFx+/qgGAx7YeHnUcxb/ddRUPv/QW7/QMcssVbvfTV5/bQ99QlOub\n5nDvhkt4X1NtIo8BWE7DmAliORAfmwp1r5LHYqTXUkrPUaTXkrppxVw+flVDSj0my2sYU3iWA5kC\npkLdq+ScRHotpfQcRXotqRf3dqTMZ2J5DWP8xQKIj02FulfJOYn0WkrpOYr0WlLXLKkZUY/J8hrG\n+IflQHxs5cKZPPjBJr75y3209wwSvxgZy/zkDuRUO6vWmyu9oy/sjQNRYjG3GOGcaWXUXFDKG209\nhAIOH1+zkPaewUQOZPYFpbx+9BQNsyqoKg+xflkdzza3saW1kzlVZSytreLRO9ckxmbUV1fyxZuX\ns6m5jaa6aVSVh5hWHkrso7MvzJ723hH1mCyvYYw/WA7Ex/w+BiReADESUxxHiCS184s3L2fbgc6U\n+TIc4K6rG0fkMJbWVlmewxgfsRzIFOD3MSAxPZPjiKS1c1Nz24ixHjFG1vPa5F2hWJ7DmMnHAoiP\n+X0MiCNnchzBtHauX1Y3YqyHw8h6XuuX1Vmew5hJynIgPhOve3W8Z5BFsyuZXh6iqy9c0PpXcUEH\n7lzbyBttPWw7eJIFM8o5dmqQvnCUC0oCfO4Dl3K4s48fbH+bmCqzq8q47l0XUlUeYmZFCb/c807K\neI3OvjAr5k9nf8fpxNiPrv4wd1/dSEtbD9WVJYmrC8tzGDP5WA7ER3Yc6uKWjS+P6A6aSAEHAo5D\nJBojvXJIwBH+8sZlPPCT3STfXZxcjypbDsPyHMZMHpYDmYS2tHYWNXiAO3f4cGRk8HBfUzY1t5E+\nNCW9HpXlOYw5P1gA8ZE1jdUjcgkTLeC4eYhMpa0CjrB+WR3pQ1PS61FZnsOY84PlQHwiPi/Fnb+z\niP/Y3cZAOEL9zArebOthKA9XJdnGjgQdKA0GCEdjzCgP8dn3LeVwZ19iXvA32nroGYhw8YWV/OVN\nywG49l1zeOPYKYajMaaVhaivruSBG5oSOYultVW0nuhL5HEsz2HM1GQ5EB9In9u7mBwhY/dVSdDh\nwQ828eBTzYn5OZLFa14lz8kRcCTlNmTLcxgzOVgOZBJJn9u7mLJN6xGvZZVtXEpyzavE8wxjQ4wx\nU4cFEB9In9u7mLJN6xGvZZVtXEpyzavE8wxjQ4wxU4flQHxg5cKZiRyCAHuP9zKzooSuvjDHe4fG\nfWXiCFSWBOgdio54LSCwfN509h7vJaqwflkttdPKEjmQ1490c3ooyppFs1haW8UfrFrAzkNdHO0e\nQByoqSzljrWNLK2tyjgnx572XstzGDNFWQ7EB+I5kMFhf5RuD3h5kPR/GdnyI5bbMGZqsRzIJBLP\ngfhFNEPwgOz5EcttGHN+sgDiA/EciF+qXgUk8z8MR8jYRsttGHN+shxIkew41MWPdx6ho3eI2VWl\nXNkwi52HuxgajhKJwbwZZZzsC9M/zm4twS106EDG8SQBgaqyEJFojKgqVzTMYnA4yuGT/SyZU8XO\nQ130D0e5aHYlVzZWo8C00iDP/+YdUOWOtY3WfWXMecpyIEUwmec6Lws5PHrnGlYunFnsphhjCmTS\n5kBE5Msi8hsReV1EnhCRGUmv3Sci+0Vkj4hcX8x2jsdknut8OBJjS2tnsZthjPEB3wUQ4Dlgmar+\nFrAXuA9ARC4FbgWagHXAN0UkULRWjsNknetccMd5xG/VNcac33yXA1HVnyctbgE+7D2/EXhcVYeA\nAyKyH7gSeHmCm5gXf7ByPh29Q3T3hxmKxBBgz/HTDA1HiQEl3iC8TGVDzoWDe0dVKOBOPZt8J1XA\ngdKAQ0kwQGVZkEvrpnHt0gt5cc87iTpWzUdPgQjL5k6jsy/M+mV11n1ljAF8GEDS3AF833s+Dzeg\nxB3x1k0q8TEf4UiMoCMgknHujfEGjrh4R1mm/UVj0B+L0T8co3tgmM7TQ9z93ou4bXV9ajsDDoc7\n+4jElFcOnmRpbZUFEWNMcbqwROR5EWnO8LgxaZv7gQjw6Dnu+y4R2S4i2zs6Os7+hgmWMi9GVLPO\nvVEMyfmN9Pk7kmtdWQ7EGANFugJR1etGe11EbgduAH5Pz9wmdhRYkLTZfG9d+r43AhvBvQsrH+3N\np/iYj+FIjMAoVyATLT2/kdJOr8JuNKaWAzHGJPiuC0tE1gF/BrxXVfuTXnoKeExEvgrMBRYD24rQ\nxHGJ1736/iuHKQ06KNDWPUDvUARVUFX6h2OEHGEoDxV6Aw7EYlBVGsAJOJzqH07sszQgLJhVwbJ5\n00fkN5Lrc8UHCsafW/eVMQZ8GECAbwClwHMiArBFVe9W1RYR+QHwBm7X1j2qOrI6oM/tONSVdU6N\nZNE8XZJEvSRIz1AUSD1dQ1Gl9UQfh0/2j8hv7DjUxV/8tIVwJMbWAycTc3xYDsQYE+e7AKKqF4/y\n2heAL0xgc/JuS2tn1jk1iiGei1HO5DdWLpw5IgcCjNjGGHN+m3yDESa5NY3VWefUKAZHUufySM+B\njJjjw3IgxhiP765AppL0elfL5k7nxT3vUDOtDFSZN6Oc4z1DtPcMoqoEHIcLSgN09Q9TEhAG8jDF\nbWlAcBxh6Zwqjp0aoPN0OFGqvTzksHZxDRfNrqSlrYemumkpd1i9Z3EN7/QMcssV9Yl50tc11drV\nhzEGsFpYBZNLvSshc9n0YnIEggGHWCxGvOkBR1JyMjb/hzFT26SthTVV5FLvym/BA0jkPJKbnp7Q\nt/k/jDFgAaRgcql35Z9MyJm2OF6eI7npAcfmNjfGjGQ5kAKK17sCUnIgLW09oErT3Om80zPIb473\nMruyhEhMCUfcsiIxzU8XV3nIQbygEIsqfUPRRHmTTDmQqvJQIkn+8EtvJXIggM1tboxJYTmQAkiu\nI1USTJ0/Y8ehLj668eW81boaL0cg6LiFFpPbOtpnMMZMbZYDKaL0MRTJdzb5dRxIeltH+wzGGAMW\nQAoifQxF8riJyToOxMZ+GGPSWQ6kANLrSKV3/axYMIPmYz0EHKEs6NDZF8YRKAkGUFUG8zD+Iy7o\nQMBxKCtxGB6OMTh8ZlT5BaUB7ttwKTCyztXZPoMxxlgAKYDkOlLp9aVu2fgykaQurF7vZ0whEs5/\naa9IDCKxGEMZbinuHYry+Sd3J3Ig2WphWf0rY0wm1oVVANnyB1taO1OChx9YDsQYM1YWQAogW/5g\nTWM1QR/lP8ByIMaYsbMurAKI5w8e+XUrg8NRvrTpTXoHIxw62UdpwGF2RZATfWGiMUU4M+1s0BFE\nIOJVx82HoAMzKkqoLAlyvGeQmCoVJQFODUQQgQ/99lyuXFTNpua2lFpYKxfO5ParGqz+lTEmKxsH\nUgA7DnVxy7f/D2epZOIbASFRYNFN5jvcflUDD29uTWxj9a+MOX/YOJAi2tLaOWmCB0BUz4x4j+c8\nnm1pT9nG6l8ZY9JZACmANY3VnKUMlq8E5Mw/hHgtrHVNtSnbWP0rY0w6y4HkQfq8H9NKg1SVBenu\njxBw4ILSIEORGAPDMQQQr8hV+kVKRcghHI3l9erFAcpLAoQCwuBwzE2YO0L3QISgA3eubaS+ujKR\nA4nXwlq5cGZivdW/MsZkYjmQccpl3g8/u3rxbLYdPGk1r4wxCZYDmSC5zPvhZ/HgYeM9jDHnygLI\nOM2sKMFx/DW241xc2TDLvX0Yd94PG+9hjMmVBZBxiJf7cOczFy6prRox+ZLfXVo3zUvKcOanMcbk\nwALIOCSX+0CV2VWlTLac0rMt7USibvHGaNS6sIwxubMAMg7p5T7WL6ujZDLdvwusa6q1kiXGmDGx\n23jHIb3kefxW10d+3cqhk/0MRzUv09KOlSNwYVUpc6aVsWh2JZ19YZrqpvHS3g7e7urnukvmcO+G\nSwCsZIkx5pxZABmH9JLnAA/8ZHfKOI5idmjFFNp7hmjvGeLNth4e/NCylPY9uetYys+HN7dSX11p\nYz6MMTmZXP0tPpNe8nxTc5tvS5gMRzVj+17c25GybCVLjDG5sgAyDplyIH5NgYQCkrF91yypSVm2\nkiXGmFxZF9Y4ZMqBHO7s47tbDtIfzn4p4uCWMRHvEd8yIG6XVyxP/V4ClIccHEe47pI53La6nqW1\nVdzzrzvoOD3E8nnT+dqtlyXKuVvJEmPMubAAMg7pOZDDnX0pJdCziQcMJTVHku/JChXo9+ZAf3LX\nMWqnldHeM0h77xAAu46c4jOPv8rXbr3MAocx5pzlFEBEZAnw34GFye9R1d8tULsmhfQcSHoJdL95\ntqWd7oHhlHXpORBjjMlVrj32PwR2Ap/HDSTxR8GIyJ+IiIrIbG9ZROTrIrJfRF4XkcsLefxcpOdA\n0kug+826ptoROY/0ZWOMyVWuXVgRVf1WQVuSREQWAO8HDietXg8s9h6rgW95P4smUw7kjbYefrXv\nxJhv343nR8YjPvYkFBDmVJVyOhzlmiU1iTEf4F55XLOkhq/detk4j2aMOV/lGkD+Q0T+K/AEMBRf\nqaonC9Iq+Fvgz4CfJK27EfiuurVCtojIDBGpU9Wi3XeangPZdqCTzftOjGuf+bgLOB68hqPKke5B\nBLf7asehLlYunGlBwxiTF7kGkD/0fiZ3WynQmN/mgIjcCBxV1dcktbjfPODtpOUj3rqUACIidwF3\nAdTXFzYxnJ4D8Ws+QTlTqt1Gmhtj8iWnAKKqi/J5UBF5HsiUMLgf+Bxu99WYqOpGYCO4E0qNdT+5\niOdAhiMxQkGHa5bUJEZ1+4mD1bkyxuRfrndhhYBPAVd7q14Evq2qw1nfNApVvS7LcZYDi4D41cd8\nYKeIXAkcBRYkbT7fW1c06TmQpbVVbGntpL1n6OxvLoDSgDC9IkQ4EiMcVWZWhLiiYRadfWHWL6uz\nqw9jTF7l2oX1LSAEfNNb/ri37s58NkZVdwMXxpdF5CCwSlVPiMhTwKdF5HHc5PmpYuY/IDUHsrW1\nk0hM8zYIcCyGoso7veHEcn84ytFdxxDglYMnWVpbZUHEGJM3uQaQK1T1t5OWfyEirxWiQaN4BtgA\n7Af6gU9M8PFHSMmBRLWohRNHYzkQY0wh5BpAoiJykaq+BSAijUC0cM1yqWpD0nMF7in0Mc9Fcg4k\n4EjRr0CysRyIMaYQcg0g/x34pYi04g4zWIgPrgAm0mNbD7OpuY2mumn0DkVQYNnc6Vy9uIbWjtPM\nqizheM8Qh0/2j+lKpDQgRBSiXgQKSPbSJpleKw06XFJbxelwlO4BtxtrRlmIZfOmWw7EGFMQud6F\n9YKILAaWeqv2qGpxMsVF8NjWw3zuid0A/CrbOI+OvnEdYygtIoxWFyvTa0ORGLuOnEpZ19kbZn9H\nn+VAjDEFMWoAEZHfVdVfiMjvp710sYigqj8uYNt8Y7LOkaFJPy0HYozJt7NdgbwX+AXwwQyvKXBe\nBJD1y+qyX3n4WLykieVAjDGFMGoAUdX/13v6F6p6IPk1Ecnr4EI/W1pbxfsvncPxnkGuaqxOyYE8\n+eoR9r9zmtrpZZw8HU6USs9VwBGqSgPUTS9nOBqjqz9MV98wIu68IOm9VQ5ujav0Li9HYO70MspK\ngqCayH1UV5ZYDsQYUxC5JtF/BKRXv/13YGV+m+M/Ow518bHvbCEciVESdHjgg02JL+LHth5m28Eu\nAE72j2lMJdGY0j0QoXugN/WFLDmQGCPzJeAGmyPdg4nleO5DwXIgxpiCOFsO5F1AEzA9LQ8yDSgr\nZMP8Ir3eVXIewe+5EcuBGGMK6WxXIEuBG4AZpOZBeoE/KlSj/CS93lVyHsHvuRHLgRhjCulsOZCf\nAD8RkatU9eUJapOvrFw4k9uvauDZlnbWNdWm/AV/2+p6th3oTMytcbIvPKZy7vEv+vTn2bYNBYRw\nWjdWechh/oxyOvuHKQ85iRpYAhw+2T+i7cYYM1655kBeFZF7cLuzEl1XqnpHQVrlI49tPZyY5/zh\nza3UV1cm5g9/bOvhRPXd8VTh1SzPs22bHjwABoZj7Esai3I0rT3pbTfGmPHKdUrb7+GWX78eeAm3\nEm7vqO+YItLzHMnLfs+BpJts7TXG+FuuAeRiVf0fQJ+q/gvwAYo8nexEWb+sLuty+mt+N9naa4zx\nt1y7sOL3qHaLyDKgnaSy61NZ8hiQW66oT3QB7TjURcuxU8yqCHGyfxjHS16MZUrabHmPbOuDDoQc\nh4HImaPdfXUj7T2D/KzlOJWlAdZePHvEOBDrvjLG5FOuAWSjiMwEPg88BVwA/I+Ctcon0seALK2t\nSqz/6D+46+PGU4U321uzrY/EIBJLDVXxPA3AwHCUJ715QEpDDo/eucYS6MaYvMu1C+sFVe1S1c2q\n2qiqFwI/L2TD/CDTGJD4+uHIWK41Jlby+A9jjMm3XAPIjzKs+/d8NsSP4mNAApI6jmJNYzWhYK6n\nrnhs/IcxppBsJPoo0uc8T+4GurimkgMn+ojFNGNpkVwFHYjGQATKQgGi0diI/YmAJq0qDQihgMPp\ncDSxjzvXNvLWiT7ePHaKslCAuTPKbfyHMaagbCT6KJLnPI/XkgL4yLdfTkz8NF7xnjBVdw7zTDTt\nUENRZSh6ZttoDB753wcS0+oKbi0ssPEfxpjCsZHoo8iWA8lX8MgXJXVO9vTWbWpuswBijMm7XO/C\n2i8inwMakt8z1UeiZ6uDFXDEV0EkXt4kElVijLz918Z/GGMKIdcA8hPgV8DzQOZ+likoWw7kj9Yu\n4ntbDjEcjXFhVSltpwYzzt2Rq3gAqJlWxmA4QmffmdLwAYFgwGEo6a6vqtIAAUfoHojgACsbZnLz\nZfPp6g+z73gvu97upn5WBQo2/sMYUzC5BpAKVf3zgrbEhzLlQPa096aMuUieg2OsFLdbrP3U4Igr\nm6hCNO2W4d6hMzE8BrxysItXD3dx59rGRE2ug539fPHm5RY8jDEFk+u9qD8VkQ0FbYkPZcqBFKqe\nVFTHl1uJxODZlvaUdVb7yhhTSLkGkD/GDSKDItIjIr0i0lPIhvlBpnEghconBMTNrYxV0IF1TbUp\n6yz3YYwppJy6sFS1qtAN8aNMOZCVC2ey7UAnT+9uIxpTBPfqYTwEKA06DMc0kWBygGBAiKkSjaXm\nVypCDpWlQU6cDlMSEK5eeiF3v/ciVi6cSX11ZaK91n1ljCmknAKIiAjwMWCRqv6liCwA6lR1W0Fb\nV2TZciDjmfsjEwX6h1PzHDEyz/uBt23/cBhwx4RcNLsykeC/bXW9BQ5jzITItQvrm8BVwG3e8mng\n7wvSIh+ZyBzIeKTnPowxZiLkGkBWq+o9wCCAqnYBJQVrlU9MZA5kPNJzH8YYMxFyng9ERAJ4XfEi\nUsPYpr6YVLKNA6mdVkp7z1BejxUQcBxh2Ou2Kg85lIcCdPcPjzjRIUdoqK6goy/MNUtquHfDJXlt\nizHG5CLXAPJ14AngQhH5AvBh3LlBprT0HAjA55/cPa65P7KJKkSTch4DwzEGhjPH6OGYsq+jD8Ht\nvtpxqMsKJhpjJlxOXViq+ijwZ8BfA23ATar6w0I1SkT+m4j8RkRaRORvktbfJyL7RWSPiFxfqOPH\npedANjW3FSR4jJXN92GMKaazlXOfpqo9IjILeAf4t6TXZqnqyXw3SESuBW4EfltVh0TkQm/9pcCt\nuOXl5wLPi8gSVS1YaZX0Wljrl9Xxv/ef8E0QEWy+D2NM8ZytC+sx3HLuO0gdihCv19dYgDZ9CnhI\nVYcAVPUdb/2NwOPe+gMish+4EihYleDkHEhT3TSefPUI5aEAfVnKrp8LAaovKCEWU04PRVjTWM0f\nX7eEP/nBLo51D7CmsZpZlSU8s9u966umqpSaC0rZc7yXqrIg775odmKuc+u+MsYUw9nKud/gjQF5\nr6oenqA2LQHe4+VaBoE/VdVXgHnAlqTtjnjrCiY5B/KrfSfyum8FTpx2x3I4AtsOnuS5lnYOdvYD\nsDnteEe7BznaPYgjEO0f5pnmdiLRM+NTLIgYYybaWZPoqqoi8jSwPF8HFZHngUz3nt7vtWkWsAa4\nAviBiOR8pSMidwF3AdTXj29AXXIOpJDiOZZcxnPEFO9OLU3JgVgAMcZMtFzHgewUkSvydVBVvU5V\nl2V4/AT3yuLH6tqGe7vwbOAosCBpN/O9den73qiqq1R1VU1NzbjaGc+BjKNEVU4cb5xJLuM5HHFL\nv4cyzNVujDETKeeBhMAWEXlLRF4Xkd0i8nqB2vQkcC2AiCzBHbB4AngKuFVESkVkEbAYKGgplXgO\nZO70MqpKA8yqCBEMjC+aOEDAcQNBRUmAxTWVlAYDXFAapL66ksU1lW5+pDLETSvmUlUaoCzkMG9G\nGVcvnk39rAru+J1FPPjBJt598WweuKHJrj6MMUUhmj7hdqaNRBYCM4H3eKs2A92qeijvDRIpAR4B\nVgBh3BzIL7zX7gfuACLAZ1R102j7WrVqlW7fvn3MbdlxqCuv85/nUyjgzopYEnR49M41FkSMMXkj\nIjtUddXZtsv1CuQm4Hu4XUk13vMPjb152alqWFX/k9eldXk8eHivfUFVL1LVpWcLHvmwpbXTl8ED\n3DxI+lztxhgzkXIdif5fgDWq2gcgIl/CvX327wrVMD9Y01jtu/nP40IBIRZTy4EYY4om1wAipM6F\nHvXWTWkrF85MzH8+FIkyvTxEOBKjfzhKdByVwBxx76YKBYSmumkc7hqgPORwz7WL+adft7K/o4+q\nsgBrGmerOp1fAAAV/ElEQVTT1R+mrXsABaaXhwgFHG65op7DnX0829LOuqZa674yxhRFrgHkn4Ct\nIvKEt3wT8I+FaZJ/PLb1cMr85519w3nZb/yCZjiq7D52ih988t2sXDiTx7YeZl9HHwA9g1Gee+M4\noaBDLBYjEnPHgpQEHQ539iXa9fDmVuqrK20OEGPMhMu1FtZXgU8AJ73HJ1T1a4VsmB9MxNwf0RiJ\nHEb68eLjPCJJVzuZxov4cY4SY8zUl2sSHVXdqapf9x6vFrJRfjERc38EHBI5jPTjxWtdBZN+S5nG\ni/hxjhJjzNSXaxfWeem21fVsO9DJpmb3L/6ZFSH6hiL0haPjGp0eCgiqSmVZkNUNbvB4bOthNjW3\nsWL+dA6e7Oe35k1ndWM1axqr2dPey/dfOcycaWV80uY+N8b4RE7jQCar8Y4DeWzrYT73xO48tiiz\nbHd6ffHm5SytreJj39lCOBKzMR/GmAmR73Eg56WJyi1ku014U3NbxnnZjTHGDyyAjGKicguBLMW2\n1i+ryzgvuzHG+IHlQEYRz4E8vbsNVWXZ3OmcHopw4EQf0TH0/DkAAmVBh4qyIKUBh6a50/nkey9i\nT3svm5rb6B0Y5uDJfq5ZUpPIbWSal90YY4rNAsgoHtt6mCd3HUss7zpyalz7iwEo9A/H6B925wLp\n6B3ik++9KBEs4jmXJ3cd48pF1SytrUqZl93m/jDG+IV1YY1iInIgw1HNOg7EciDGGD+zADKKiciB\nhAKSdRyI5UCMMX5mXVijyHcOJCAgIpSHHEpLAswoC3HH2sZEl1T8eC/u7bAciDHG9yyAjCLfORB3\nJlqldyhK71CUE71hHnyqOZHXSD6e5UCMMX5nXVijsByIMcZkZwFkFJYDMcaY7KwLaxTxnMR/vN6G\nxpSLaipRsByIMcZgAWRU6TmQ+FwdY2U5EGPMVGJdWKOwHIgxxmRnAWQUlgMxxpjsrAtrFLetrufZ\n5jZ+vf8EAiyfZ+NAjDEmzgLIKB565k027zuRWLZxIMYYc4Z1YY0ife7xQrAciDFmsrIAksXah17g\nYGd/wY8z2pzo8RxI0BEEd94Qy4EYY/zCAkgGax96gSPdgxNyrJxSKSKpP40xxgcsgGRwdIKCB0A0\nxlm7sCLRGApEo9aFZYzxDwsgGcybUTZhxwrm0IVlt/EaY/zIAkgGv7739wq2b0egJCBUlQaYVRHi\nzrTbeG9aMZcZFSFuWjGX21bXs3LhTB64oYl3XzybB25osjuwjDG+YbfxZrDk/mcKtu+YQjiqhKNR\nIMrDm1upr67kttX1dhuvMWZSsSuQDMJjGSU4DvHch93Ga4yZTHwXQERkhYhsEZFdIrJdRK701ouI\nfF1E9ovI6yJyeaHaUBKY2Lud4rkPy4EYYyYT3wUQ4G+A/6mqK4AHvGWA9cBi73EX8K1CNWDvFzYU\nZL8ClAYdQgGhtqqUiy+8gCsbZtJ87BSPbT1MV3+YFfOnUxIQGqorEt1Vt1/VwIJZFdx+VYN1Xxlj\nfMOPORAFpnnPpwPxeuo3At9VVQW2iMgMEalT1byXzF1079P53iXgfrChSAyA9t4h6B0CYNvBrhHb\nHuzs5yPffpk/WruIhze3AqTkS4wxptj8eAXyGeDLIvI28L+A+7z184C3k7Y74q1LISJ3eV1f2zs6\nOsbUgInNgGQXjemIcioTUWLeGGNyUZQAIiLPi0hzhseNwKeAz6rqAuCzwD+ey75VdaOqrlLVVTU1\nNWNr35jelX8BR1jXVJuybiJKzBtjTC6K0oWlqtdle01Evgv8sbf4Q+A73vOjwIKkTed76/LuwEMf\noKEA3VhBB0oCDkNRpaayhOULZnCqP8xQJMai2ZV09oVp7x7g0Ml+5s4o5ysfWcHKhTOpr65MlHO3\n7itjjF/4MQdyDHgv8CLwu8A+b/1TwKdF5HFgNXCqEPkPoCDBAyASg0jsTA6k/Y3jgHvF81paqfiD\nnf3sae9l5cKZ3La63gKHMcZ3/BhA/gj4/0QkCAzi3nEF8AywAdgP9AOfKE7z8i9bzmVTc5sFDmOM\nb/kugKjqr4GVGdYrcM/Et6jwhMxBxPIdxhg/810A8YODBciBVFeGuH5ZHf1DEXa93c2KBTNYPKeK\nmRUldPWHEz97B4ZpaeuxfIcxxvcsgGSw9qEX8r7Pzr5hppUG+eLNy/O+b2OMKQY/jgMpukLNBzIR\nU+QaY8xEsQCSQaHmA0kf02GMMZOZBZAM8j0fSHkowE0r5nLvhkvyul9jjCkmCyAZXHRffhPoA8NR\nnmluZ8ehkTWvjDFmsrIAkkEhpgOxuTyMMVONBZAMCjEdiM3lYYyZaiyAZPDWX38gr/ubVVnCHe+2\nuTyMMVOLBZAM8j0O5GRfmIc3t/LY1sN53a8xxhSTBZAMCjUOxObyMMZMJRZAMijUOBCrbWWMmUos\ngGSQz3EgAlx84QV88eblVtvKGDOlWADJYMn9z+RtXwocONHH0tqqvO3TGGP8wAJIBuE8DwSJxtTG\ngBhjphwLIBmU5HkgSMARGwNijJlyLIBksPcLG8b83oBAaUCoCDkEHbiwqoS/vHGZjQExxkw5FkAy\nGM9kUlGFoagyFIkRicE7vWEe/I8Wq4NljJlyLIAUSHIaxepgGWOmIgsgBZKcRrE6WMaYqcimtM1g\nLHOihwJCw6wKhmPKigUz6A9H2Xm4i5jCR1bOtxyIMWbKsQCSwfu+8uI5v2c4quzr6APgYGd/ymsP\nb26lvrrSBhIaY6YU68LK4K0TfXnfp9XBMsZMNRZAMrhodmXe92l1sIwxU411YWXw3J9cw4r/+TO6\nByIZX3fEHRwYUyUUcIjGFEeES2qrqCoP0VQ3jZ2Hu3izrYfp5SH+67WLrfvKGDPlWADJ4KFn3swa\nPABiCjHvPt1oLOatVXYdOQXAltZOhr3Xe4eiBW2rMcYUi3VhZfBsS/u43j+cVkvL8h/GmKnIAkgG\n65pqx/xewb2lN5nlP4wxU5F1YWVw74ZL2NLayWtHTqG4OY+KUACAcDRG3fRyltZW8fbJfrr6w1SV\nBunoC/Nb86azurGaNY3V7GnvZVNzG+uX1Vn+wxgzJVkAyeChZ95M5DPAzXmcDp/JZRw62c+hk2fG\nerQzBMDmfSdYt6yOlQtnsnLhTAscxpgprShdWCLyByLSIiIxEVmV9tp9IrJfRPaIyPVJ69d56/aL\nyL2FbN94ciCW7zDGnC+KlQNpBn4f2Jy8UkQuBW4FmoB1wDdFJCAiAeDvgfXApcBHvW0LYjw5EMt3\nGGPOF0XpwlLVNwFERkzcdCPwuKoOAQdEZD9wpffaflVt9d73uLftG4Vo370bLgHgyV1H6RuK0DcU\nRRyoKg1SWRZiWmmQkqCDAAdP9tMwq4Kq8pDlO4wx5xW/5UDmAVuSlo946wDeTlu/upANuXfDJbT3\nDPLkrmPuihh0D0ToHohwNG3bXf2n+OLNyy14GGPOKwXrwhKR50WkOcPjxkId0zvuXSKyXUS2d3R0\njGtfL+7N/f2W+zDGnG8KdgWiqteN4W1HgQVJy/O9dYyyPv24G4GNAKtWrdJM2+TqmiU1Z65AzsJy\nH8aY843furCeAh4Tka8Cc4HFwDbc8XmLRWQRbuC4Fbit0I352q2XAfCzluOUBIWlc6qYUVECwOyq\nUqaVBmlp67HchzHmvFSUACIiNwN/B9QAT4vILlW9XlVbROQHuMnxCHCPqka993wa+BkQAB5R1ZaJ\naGs8iBhjjEklquPq5fG1VatW6fbt24vdDGOMmVREZIeqrjrbdlYLyxhjzJhYADHGGDMmFkCMMcaM\niQUQY4wxY2IBxBhjzJhM6buwRKQDODSOXcwGTuSpOYUyGdoI1s58mgxtBGtnvk1kOxeqas3ZNprS\nAWS8RGR7LreyFdNkaCNYO/NpMrQRrJ355sd2WheWMcaYMbEAYowxZkwsgIxuY7EbkIPJ0EawdubT\nZGgjWDvzzXfttByIMcaYMbErEGOMMWNiASQDEVknIntEZL+I3FvktiwQkV+KyBsi0iIif+ytf1BE\njorILu+xIek993lt3yMi109QOw+KyG6vLdu9dbNE5DkR2ef9nOmtFxH5utfG10Xk8glq49Kk87VL\nRHpE5DN+OJci8oiIvCMizUnrzvn8icgfetvvE5E/nKB2fllEfuO15QkRmeGtbxCRgaTz+nDSe1Z6\n/172e59lxPzWeW7jOf+OC/09kKWd309q40ER2eWtL8q5PCtVtUfSA7dc/FtAI1ACvAZcWsT21AGX\ne8+rgL3ApcCDwJ9m2P5Sr82lwCLvswQmoJ0Hgdlp6/4GuNd7fi/wJe/5BmAT7jwva4CtRfo9twML\n/XAugauBy4HmsZ4/YBbQ6v2c6T2fOQHtfD8Q9J5/KamdDcnbpe1nm9d28T7L+gK38Zx+xxPxPZCp\nnWmvfwV4oJjn8mwPuwIZ6Upgv6q2qmoYeBwo6DS8o1HVNlXd6T3vBd7kzDzxmdwIPK6qQ6p6ANiP\n+5mK4UbgX7zn/wLclLT+u+raAswQkYme0vH3gLdUdbSBphN2LlV1M3Ayw/HP5fxdDzynqidVtQt4\nDlhX6Haq6s9VNeItbsGdMTQrr63TVHWLut+A3+XMZytIG0eR7Xdc8O+B0drpXUV8BPi30fZR6HN5\nNhZARpoHvJ20fITRv7AnjIg0AJcBW71Vn/a6DR6Jd29QvPYr8HMR2SEid3nr5qhqfLL4dmBOkduY\n7FZS/3P66VzGnev5K3Z7Ae7A/Ss4bpGIvCoiL4nIe7x187y2xU1UO8/ld1zsc/ke4Liq7kta56dz\nCVgAmTRE5ALgR8BnVLUH+BZwEbACaMO93C2mtap6ObAeuEdErk5+0fvryBe3/IlICfAh4IfeKr+d\nyxH8dP6yEZH7cWcSfdRb1QbUq+plwP+DO131tCI1z/e/4zQfJfUPHD+dywQLICMdBRYkLc/31hWN\niIRwg8ejqvpjAFU9rqpRVY0B/8CZrpWitF9Vj3o/3wGe8NpzPN415f18p5htTLIe2Kmqx8F/5zLJ\nuZ6/orVXRG4HbgA+5gU7vG6hTu/5DtycwhKvTcndXAVv5xh+x8U8l0Hg94Hvx9f56VwmswAy0ivA\nYhFZ5P2leivwVLEa4/WF/iPwpqp+NWl9cs7gZiB+J8dTwK0iUioii4DFuEm2QraxUkSq4s9xk6rN\nXlvidwL9IfCTpDb+Z+9uojXAqaSumomQ8tedn85lmnM9fz8D3i8iM70umvd76wpKRNYBfwZ8SFX7\nk9bXiEjAe96Ie/5avbb2iMga79/3f076bIVq47n+jov5PXAd8BtVTXRN+elcppiobP1keuDe5bIX\nN8rfX+S2rMXtungd2OU9NgDfA3Z7658C6pLec7/X9j1MwB0ZuHeqvOY9WuLnDKgGXgD2Ac8Ds7z1\nAvy918bdwKoJPJ+VQCcwPWld0c8lbkBrA4Zx+7H/y1jOH24OYr/3+MQEtXM/br4g/u/zYW/b/8v7\n97AL2Al8MGk/q3C/xN8CvoE3qLmAbTzn33GhvwcytdNb/8/A3WnbFuVcnu1hI9GNMcaMiXVhGWOM\nGRMLIMYYY8bEAogxxpgxsQBijDFmTCyAGGOMGRMLIOa84lU1bc5hu38WkQ97z78jIpeOsu3tIjI3\nn+3MlTc+YKtX4uI9Z3/HuI5V7pXRiI9HeFZEukXkp2nbLfLatN+rLluSYV8iGSoKe5/n2UJ+DpM/\nFkDMlBD/UisEVb1TVd8YZZPbgaIEENyikLtV9TJV/VXyCwU4J3cAP1bVqLf8ZeDjGbb7EvC3qnox\n0IU7DiPdetzBcIuBu3BLjaCqHUCbiPxOnttuCsACiCkIb3T60yLymog0i8gt3vp14s4dsdP7C/Sn\n3voHReRPk97fLG7xSETkSXGLNLbImUKNiMhpEfmKiLwGXCXuvAgvedv+LKkMyEqvHa8B92Rpr4jI\nN8Sd/+F54MKk114UkVUiEvCuTJrFnX/hs95VyirgUXHnaSgXkQdE5BVvu43eCOH4fr4kIttEZG/8\nisHb7//ytn9dRP5bUrtHfJ6kdq3ALfl+Y9Kx08/J73lXJ7vFLSJY6r33oIj8tfe+7SJyuXeMt0Tk\n7iy/1o+RNMpZVV8AetPPI/C7wL97q5KrCCcbrSLzk96xjN9N1IhFe5xfD9yRs/+QtDwdKMMdsbwY\ndzT1D4Cfeq8/SNJ8Dbgjaxu85/ER2OXe+mpvWYGPeM9DwP8BarzlW4BHvOevA1d7z79MhnkVcGsP\nPYc7D8RcoBv4sPfai7hBYiVuufT4e2Ykv560flbS8+/hjRr2tvuK93wD8Lz3/FO4X7jxOTVmjfZ5\n0tp9O/CNpOXkcxI/30u85e/iFuMEd/6WT3nP/9Y7R1VADW4V2PTjlADtGdZfE/8desuzccugx5cX\nZDnfP8UtwBlffiF+DnGrye4u9r9he5z9YVcgplB2A+/z/uJ+j6qeAt4FHFDVfep+U/xrjvv6v72/\nqLfgfiEt9tZHcYtMAiwFlgHPiTuL2+eB+eLOjjdD3bkXwP1Cz+Rq4N/ULbh3DPhFhm1agUYR+Ttx\n6z/1ZNnXtV4OYDfuX+NNSa/92Pu5A3eSIHBrH31bvTk1VPVkts+T5XjJ0s/JAVXd6y3/i/c54+K1\nnXbjTkrVq24X0pB33pLNxg2qE+EditclaM5BsNgNMFOTqu71EqMbgL8SkRcYvRhdhNQu1TIAEbkG\n9wv2KlXtF5EX468Bg3qmP16AFlW9KnmnGb4Ix0xVu0Tkt3Enbrobd8KfO9KOVwZ8E/ev6bdF5MGk\n9gIMeT+jjP7/L+PnyUHyOTmbeFtiSc/jy+ltGyD1c2TTidsdFfQCYrbqsKNVuy3zjmd8zq5ATEGI\ne1dSv6r+K2630eXAb4AGEbnI2+yjSW856G2DF3gWeeunA11e8HgX7tSdmewBakTkKm8fIRFpUtVu\noFtE1nrbZetb3wzc4uUj6oBrM3ym2YCjqj/CvSKIz0Xei9v9A2e+ZE+IO4fLh7McL9lzwCfFLeON\niMzK9nly2FeyPbjn+2Jv+ePAS+e4D8ANnkDAC5CjbafALznzuRNVhEXkZhH5a2/9aBWZl3CmWq7x\nMbsCMYWyHPiyiMRwq41+SlUHvST40yLSD/yKM1+8P8L9QmnBnXEx3u3yLHC3iLyJ+4W4JdPBVDXs\nJbS/LiLTcf9tfw23gukngEdERIGfZ2nvE7jdTW8Ah4GXM2wzD/gnEYn/4XWf9/OfgYdFZAC4Cne+\niWbcWQRfyXK8ZN/B/dJ8XUSGcXNH3xjl8+TEO9+fAH7oBadXgIdzfX8GP8etDv08gIj8Crdb8gIR\niVeT/Rnw58DjIvJXwKu40xGAO6FTvNvvGdyr0/1AP+7vKO5a4OlxtNNMEKvGa4rG6576U1W9odht\nMWfnXRl+VlUz3bqby/v/1Xt/x1m22wzc6F31GB+zKxBjTE5UdaeI/FJEAueQZ0l+/3862zYiUgN8\n1YLH5GBXIMYYY8bEkujGGGPGxAKIMcaYMbEAYowxZkwsgBhjjBkTCyDGGGPGxAKIMcaYMfn/ASum\nzLfOU2KsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff3cb1c7e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9595, 5757)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "net = \"orientation_regression\"\n",
    "action = {u'UP': 0, u'DOWN': 1, u'LEFT': 2, u'RIGHT': 3, u'No_Control': 4}\n",
    "data = preprocess_raw(\"dataset_basic_103_trips.npy\")\n",
    "print(\"freq of each action after preprocessing: \",action_count(data[:,-1].astype(int)))\n",
    "\n",
    "## visualize orientation of car vs its distance from (10,0)\n",
    "orientation_vs_dist(data[:,0], data[:,1], data[:,17])\n",
    "\n",
    "X, Y = extract_features_labels(data, net)\n",
    "train_test_ratio = 0.6\n",
    "train_max = np.round(len(X)*train_test_ratio).astype(int)\n",
    "print(len(X), train_max)\n",
    "X_train = X[:train_max,:]\n",
    "Y_train = Y[:train_max,:]\n",
    "X_test = X[train_max:,:]\n",
    "Y_test = Y[train_max:,:]\n",
    "\n",
    "if net == \"orientation_regression\":\n",
    "    ## since we need to do polynomial regression, we extract polynomial features\n",
    "    ## note that linear regression might work too but not with a great accuracy\n",
    "    ## try by commenting this section\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    X_train = poly.fit_transform(X_train)\n",
    "    poly_params = poly.get_params()\n",
    "    np.save(\"poly_params.npy\", poly_params)\n",
    "    X_test = poly.transform(X_test)\n",
    "\n",
    "X_train, X_test, min_scale, max_scale = min_max_scaling(X_train, X_test)\n",
    "np.save(\"min_scale.npy\", min_scale)\n",
    "np.save(\"max_scale.npy\", max_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining model\n",
      "Setting up data\n",
      "Training model\n",
      "Train Epoch: 0 [0/5757 (0%)]\tLoss: 1161.135742\n",
      "Train Epoch: 1 [0/5757 (0%)]\tLoss: 21.244522\n",
      "Train Epoch: 2 [0/5757 (0%)]\tLoss: 10.663212\n",
      "Train Epoch: 3 [0/5757 (0%)]\tLoss: 12.084784\n",
      "Train Epoch: 4 [0/5757 (0%)]\tLoss: 6.467068\n",
      "Train Epoch: 5 [0/5757 (0%)]\tLoss: 4.267134\n",
      "Train Epoch: 6 [0/5757 (0%)]\tLoss: 8.739014\n",
      "Train Epoch: 7 [0/5757 (0%)]\tLoss: 12.854810\n",
      "Train Epoch: 8 [0/5757 (0%)]\tLoss: 7.364078\n",
      "Train Epoch: 9 [0/5757 (0%)]\tLoss: 6.651398\n",
      "Train Epoch: 10 [0/5757 (0%)]\tLoss: 2.790190\n",
      "Train Epoch: 11 [0/5757 (0%)]\tLoss: 4.334650\n",
      "Train Epoch: 12 [0/5757 (0%)]\tLoss: 5.231684\n",
      "Train Epoch: 13 [0/5757 (0%)]\tLoss: 5.164523\n",
      "Train Epoch: 14 [0/5757 (0%)]\tLoss: 5.970358\n",
      "Train Epoch: 15 [0/5757 (0%)]\tLoss: 4.836848\n",
      "Train Epoch: 16 [0/5757 (0%)]\tLoss: 6.639667\n",
      "Train Epoch: 17 [0/5757 (0%)]\tLoss: 3.257738\n",
      "Train Epoch: 18 [0/5757 (0%)]\tLoss: 4.826727\n",
      "Train Epoch: 19 [0/5757 (0%)]\tLoss: 9.654179\n",
      "Train Epoch: 20 [0/5757 (0%)]\tLoss: 4.785578\n",
      "Train Epoch: 21 [0/5757 (0%)]\tLoss: 1.421188\n",
      "Train Epoch: 22 [0/5757 (0%)]\tLoss: 3.899977\n",
      "Train Epoch: 23 [0/5757 (0%)]\tLoss: 3.163408\n",
      "Train Epoch: 24 [0/5757 (0%)]\tLoss: 4.150046\n",
      "Train Epoch: 25 [0/5757 (0%)]\tLoss: 2.159791\n",
      "Train Epoch: 26 [0/5757 (0%)]\tLoss: 10.300476\n",
      "Train Epoch: 27 [0/5757 (0%)]\tLoss: 6.211246\n",
      "Train Epoch: 28 [0/5757 (0%)]\tLoss: 2.609853\n",
      "Train Epoch: 29 [0/5757 (0%)]\tLoss: 5.537078\n",
      "Train Epoch: 30 [0/5757 (0%)]\tLoss: 5.426021\n",
      "Train Epoch: 31 [0/5757 (0%)]\tLoss: 2.479384\n",
      "Train Epoch: 32 [0/5757 (0%)]\tLoss: 4.487547\n",
      "Train Epoch: 33 [0/5757 (0%)]\tLoss: 3.039471\n",
      "Train Epoch: 34 [0/5757 (0%)]\tLoss: 6.257946\n",
      "Train Epoch: 35 [0/5757 (0%)]\tLoss: 7.504222\n",
      "Train Epoch: 36 [0/5757 (0%)]\tLoss: 1.985239\n",
      "Train Epoch: 37 [0/5757 (0%)]\tLoss: 4.361052\n",
      "Train Epoch: 38 [0/5757 (0%)]\tLoss: 3.356422\n",
      "Train Epoch: 39 [0/5757 (0%)]\tLoss: 0.702062\n",
      "Train Epoch: 40 [0/5757 (0%)]\tLoss: 3.144141\n",
      "Train Epoch: 41 [0/5757 (0%)]\tLoss: 4.839722\n",
      "Train Epoch: 42 [0/5757 (0%)]\tLoss: 9.200586\n",
      "Train Epoch: 43 [0/5757 (0%)]\tLoss: 5.214736\n",
      "Train Epoch: 44 [0/5757 (0%)]\tLoss: 0.717499\n",
      "Train Epoch: 45 [0/5757 (0%)]\tLoss: 2.123672\n",
      "Train Epoch: 46 [0/5757 (0%)]\tLoss: 1.273312\n",
      "Train Epoch: 47 [0/5757 (0%)]\tLoss: 1.373465\n",
      "Train Epoch: 48 [0/5757 (0%)]\tLoss: 2.774203\n",
      "Train Epoch: 49 [0/5757 (0%)]\tLoss: 1.879381\n",
      "Train Epoch: 50 [0/5757 (0%)]\tLoss: 5.862374\n",
      "Train Epoch: 51 [0/5757 (0%)]\tLoss: 2.030092\n",
      "Train Epoch: 52 [0/5757 (0%)]\tLoss: 4.310997\n",
      "Train Epoch: 53 [0/5757 (0%)]\tLoss: 2.849066\n",
      "Train Epoch: 54 [0/5757 (0%)]\tLoss: 7.438343\n",
      "Train Epoch: 55 [0/5757 (0%)]\tLoss: 5.422155\n",
      "Train Epoch: 56 [0/5757 (0%)]\tLoss: 4.462341\n",
      "Train Epoch: 57 [0/5757 (0%)]\tLoss: 2.049246\n",
      "Train Epoch: 58 [0/5757 (0%)]\tLoss: 2.117229\n",
      "Train Epoch: 59 [0/5757 (0%)]\tLoss: 1.556018\n",
      "Train Epoch: 60 [0/5757 (0%)]\tLoss: 0.612959\n",
      "Train Epoch: 61 [0/5757 (0%)]\tLoss: 7.604261\n",
      "Train Epoch: 62 [0/5757 (0%)]\tLoss: 2.096678\n",
      "Train Epoch: 63 [0/5757 (0%)]\tLoss: 5.575446\n",
      "Train Epoch: 64 [0/5757 (0%)]\tLoss: 4.711460\n",
      "Train Epoch: 65 [0/5757 (0%)]\tLoss: 1.512932\n",
      "Train Epoch: 66 [0/5757 (0%)]\tLoss: 1.845367\n",
      "Train Epoch: 67 [0/5757 (0%)]\tLoss: 2.158557\n",
      "Train Epoch: 68 [0/5757 (0%)]\tLoss: 4.935425\n",
      "Train Epoch: 69 [0/5757 (0%)]\tLoss: 2.994576\n",
      "Train Epoch: 70 [0/5757 (0%)]\tLoss: 5.432239\n",
      "Train Epoch: 71 [0/5757 (0%)]\tLoss: 3.568266\n",
      "Train Epoch: 72 [0/5757 (0%)]\tLoss: 3.411169\n",
      "Train Epoch: 73 [0/5757 (0%)]\tLoss: 2.971434\n",
      "Train Epoch: 74 [0/5757 (0%)]\tLoss: 4.061970\n",
      "Train Epoch: 75 [0/5757 (0%)]\tLoss: 2.142076\n",
      "Train Epoch: 76 [0/5757 (0%)]\tLoss: 6.411271\n",
      "Train Epoch: 77 [0/5757 (0%)]\tLoss: 3.459821\n",
      "Train Epoch: 78 [0/5757 (0%)]\tLoss: 2.952435\n",
      "Train Epoch: 79 [0/5757 (0%)]\tLoss: 0.952449\n",
      "Train Epoch: 80 [0/5757 (0%)]\tLoss: 2.966069\n",
      "Train Epoch: 81 [0/5757 (0%)]\tLoss: 3.487142\n",
      "Train Epoch: 82 [0/5757 (0%)]\tLoss: 4.245972\n",
      "Train Epoch: 83 [0/5757 (0%)]\tLoss: 1.348582\n",
      "Train Epoch: 84 [0/5757 (0%)]\tLoss: 6.491870\n",
      "Train Epoch: 85 [0/5757 (0%)]\tLoss: 3.489418\n",
      "Train Epoch: 86 [0/5757 (0%)]\tLoss: 2.891513\n",
      "Train Epoch: 87 [0/5757 (0%)]\tLoss: 2.892153\n",
      "Train Epoch: 88 [0/5757 (0%)]\tLoss: 1.532437\n",
      "Train Epoch: 89 [0/5757 (0%)]\tLoss: 1.298201\n",
      "Train Epoch: 90 [0/5757 (0%)]\tLoss: 6.260191\n",
      "Train Epoch: 91 [0/5757 (0%)]\tLoss: 1.906217\n",
      "Train Epoch: 92 [0/5757 (0%)]\tLoss: 2.917998\n",
      "Train Epoch: 93 [0/5757 (0%)]\tLoss: 2.395734\n",
      "Train Epoch: 94 [0/5757 (0%)]\tLoss: 5.522886\n",
      "Train Epoch: 95 [0/5757 (0%)]\tLoss: 3.800652\n",
      "Train Epoch: 96 [0/5757 (0%)]\tLoss: 1.498829\n",
      "Train Epoch: 97 [0/5757 (0%)]\tLoss: 1.432101\n",
      "Train Epoch: 98 [0/5757 (0%)]\tLoss: 3.407957\n",
      "Train Epoch: 99 [0/5757 (0%)]\tLoss: 11.273170\n",
      "Train Epoch: 100 [0/5757 (0%)]\tLoss: 2.385889\n",
      "Train Epoch: 101 [0/5757 (0%)]\tLoss: 4.429039\n",
      "Train Epoch: 102 [0/5757 (0%)]\tLoss: 1.561194\n",
      "Train Epoch: 103 [0/5757 (0%)]\tLoss: 2.258332\n",
      "Train Epoch: 104 [0/5757 (0%)]\tLoss: 3.822073\n",
      "Train Epoch: 105 [0/5757 (0%)]\tLoss: 2.839487\n",
      "Train Epoch: 106 [0/5757 (0%)]\tLoss: 1.391369\n",
      "Train Epoch: 107 [0/5757 (0%)]\tLoss: 2.904687\n",
      "Train Epoch: 108 [0/5757 (0%)]\tLoss: 2.421238\n",
      "Train Epoch: 109 [0/5757 (0%)]\tLoss: 1.356645\n",
      "Train Epoch: 110 [0/5757 (0%)]\tLoss: 1.989707\n",
      "Train Epoch: 111 [0/5757 (0%)]\tLoss: 2.056924\n",
      "Train Epoch: 112 [0/5757 (0%)]\tLoss: 0.923982\n",
      "Train Epoch: 113 [0/5757 (0%)]\tLoss: 2.092074\n",
      "Train Epoch: 114 [0/5757 (0%)]\tLoss: 3.009822\n",
      "Train Epoch: 115 [0/5757 (0%)]\tLoss: 1.851328\n",
      "Train Epoch: 116 [0/5757 (0%)]\tLoss: 4.337753\n",
      "Train Epoch: 117 [0/5757 (0%)]\tLoss: 3.132876\n",
      "Train Epoch: 118 [0/5757 (0%)]\tLoss: 4.639499\n",
      "Train Epoch: 119 [0/5757 (0%)]\tLoss: 2.362191\n",
      "Train Epoch: 120 [0/5757 (0%)]\tLoss: 5.149204\n",
      "Train Epoch: 121 [0/5757 (0%)]\tLoss: 3.673746\n",
      "Train Epoch: 122 [0/5757 (0%)]\tLoss: 2.063311\n",
      "Train Epoch: 123 [0/5757 (0%)]\tLoss: 5.208645\n",
      "Train Epoch: 124 [0/5757 (0%)]\tLoss: 1.230534\n",
      "Train Epoch: 125 [0/5757 (0%)]\tLoss: 2.680773\n",
      "Train Epoch: 126 [0/5757 (0%)]\tLoss: 2.546381\n",
      "Train Epoch: 127 [0/5757 (0%)]\tLoss: 1.956363\n",
      "Train Epoch: 128 [0/5757 (0%)]\tLoss: 1.771283\n",
      "Train Epoch: 129 [0/5757 (0%)]\tLoss: 1.238479\n",
      "Train Epoch: 130 [0/5757 (0%)]\tLoss: 2.469241\n",
      "Train Epoch: 131 [0/5757 (0%)]\tLoss: 1.909903\n",
      "Train Epoch: 132 [0/5757 (0%)]\tLoss: 2.746482\n",
      "Train Epoch: 133 [0/5757 (0%)]\tLoss: 4.033821\n",
      "Train Epoch: 134 [0/5757 (0%)]\tLoss: 2.192427\n",
      "Train Epoch: 135 [0/5757 (0%)]\tLoss: 1.660006\n",
      "Train Epoch: 136 [0/5757 (0%)]\tLoss: 1.566017\n",
      "Train Epoch: 137 [0/5757 (0%)]\tLoss: 1.724607\n",
      "Train Epoch: 138 [0/5757 (0%)]\tLoss: 2.111296\n",
      "Train Epoch: 139 [0/5757 (0%)]\tLoss: 2.418350\n",
      "Train Epoch: 140 [0/5757 (0%)]\tLoss: 4.215733\n",
      "Train Epoch: 141 [0/5757 (0%)]\tLoss: 4.202467\n",
      "Train Epoch: 142 [0/5757 (0%)]\tLoss: 1.100646\n",
      "Train Epoch: 143 [0/5757 (0%)]\tLoss: 3.580468\n",
      "Train Epoch: 144 [0/5757 (0%)]\tLoss: 4.466374\n",
      "Train Epoch: 145 [0/5757 (0%)]\tLoss: 1.557674\n",
      "Train Epoch: 146 [0/5757 (0%)]\tLoss: 2.741847\n",
      "Train Epoch: 147 [0/5757 (0%)]\tLoss: 1.492158\n",
      "Train Epoch: 148 [0/5757 (0%)]\tLoss: 2.579682\n",
      "Train Epoch: 149 [0/5757 (0%)]\tLoss: 2.039356\n",
      "Train Epoch: 150 [0/5757 (0%)]\tLoss: 4.490149\n",
      "Train Epoch: 151 [0/5757 (0%)]\tLoss: 1.669242\n",
      "Train Epoch: 152 [0/5757 (0%)]\tLoss: 3.534192\n",
      "Train Epoch: 153 [0/5757 (0%)]\tLoss: 1.033020\n",
      "Train Epoch: 154 [0/5757 (0%)]\tLoss: 1.207700\n",
      "Train Epoch: 155 [0/5757 (0%)]\tLoss: 3.839201\n",
      "Train Epoch: 156 [0/5757 (0%)]\tLoss: 1.998061\n",
      "Train Epoch: 157 [0/5757 (0%)]\tLoss: 2.616337\n",
      "Train Epoch: 158 [0/5757 (0%)]\tLoss: 3.589236\n",
      "Train Epoch: 159 [0/5757 (0%)]\tLoss: 2.386914\n",
      "Train Epoch: 160 [0/5757 (0%)]\tLoss: 3.541500\n",
      "Train Epoch: 161 [0/5757 (0%)]\tLoss: 1.153058\n",
      "Train Epoch: 162 [0/5757 (0%)]\tLoss: 2.657399\n",
      "Train Epoch: 163 [0/5757 (0%)]\tLoss: 4.915596\n",
      "Train Epoch: 164 [0/5757 (0%)]\tLoss: 2.871196\n",
      "Train Epoch: 165 [0/5757 (0%)]\tLoss: 0.995994\n",
      "Train Epoch: 166 [0/5757 (0%)]\tLoss: 0.674403\n",
      "Train Epoch: 167 [0/5757 (0%)]\tLoss: 2.450742\n",
      "Train Epoch: 168 [0/5757 (0%)]\tLoss: 1.404501\n",
      "Train Epoch: 169 [0/5757 (0%)]\tLoss: 2.749434\n",
      "Train Epoch: 170 [0/5757 (0%)]\tLoss: 1.797753\n",
      "Train Epoch: 171 [0/5757 (0%)]\tLoss: 3.702269\n",
      "Train Epoch: 172 [0/5757 (0%)]\tLoss: 1.547441\n",
      "Train Epoch: 173 [0/5757 (0%)]\tLoss: 4.109236\n",
      "Train Epoch: 174 [0/5757 (0%)]\tLoss: 1.460729\n",
      "Train Epoch: 175 [0/5757 (0%)]\tLoss: 1.112621\n",
      "Train Epoch: 176 [0/5757 (0%)]\tLoss: 1.097275\n",
      "Train Epoch: 177 [0/5757 (0%)]\tLoss: 2.044740\n",
      "Train Epoch: 178 [0/5757 (0%)]\tLoss: 3.705800\n",
      "Train Epoch: 179 [0/5757 (0%)]\tLoss: 1.736511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 180 [0/5757 (0%)]\tLoss: 1.480644\n",
      "Train Epoch: 181 [0/5757 (0%)]\tLoss: 2.761586\n",
      "Train Epoch: 182 [0/5757 (0%)]\tLoss: 2.332815\n",
      "Train Epoch: 183 [0/5757 (0%)]\tLoss: 2.445506\n",
      "Train Epoch: 184 [0/5757 (0%)]\tLoss: 4.849010\n",
      "Train Epoch: 185 [0/5757 (0%)]\tLoss: 2.276700\n",
      "Train Epoch: 186 [0/5757 (0%)]\tLoss: 3.081059\n",
      "Train Epoch: 187 [0/5757 (0%)]\tLoss: 2.403405\n",
      "Train Epoch: 188 [0/5757 (0%)]\tLoss: 4.225219\n",
      "Train Epoch: 189 [0/5757 (0%)]\tLoss: 2.848859\n",
      "Train Epoch: 190 [0/5757 (0%)]\tLoss: 0.666198\n",
      "Train Epoch: 191 [0/5757 (0%)]\tLoss: 6.071071\n",
      "Train Epoch: 192 [0/5757 (0%)]\tLoss: 1.397020\n",
      "Train Epoch: 193 [0/5757 (0%)]\tLoss: 1.816775\n",
      "Train Epoch: 194 [0/5757 (0%)]\tLoss: 5.888301\n",
      "Train Epoch: 195 [0/5757 (0%)]\tLoss: 1.334337\n",
      "Train Epoch: 196 [0/5757 (0%)]\tLoss: 3.322827\n",
      "Train Epoch: 197 [0/5757 (0%)]\tLoss: 1.654599\n",
      "Train Epoch: 198 [0/5757 (0%)]\tLoss: 2.348659\n",
      "Train Epoch: 199 [0/5757 (0%)]\tLoss: 4.951272\n",
      "Training Done\n"
     ]
    }
   ],
   "source": [
    "# model parameters\n",
    "if(net == \"mlp_all_labels\"):\n",
    "    n_hidden = 9\n",
    "    out_dimn = 5\n",
    "elif net == \"orientation_regression\":\n",
    "    n_hidden = 0\n",
    "    out_dimn = 1\n",
    "batch_size = 32\n",
    "n_train = 10000 \n",
    "n_test =1000\n",
    "n_epochs= 200\n",
    "predict_every = 20\n",
    "log_interval = 1000\n",
    "\n",
    "print(\"Defining model\")\n",
    "model, optimizer, criterion = make_model(inp_dimn=X_train.shape[1], out_dimn=out_dimn, net=net, n_hidden=n_hidden)\n",
    "\n",
    "print(\"Setting up data\")\n",
    "# Y_train = Y_train.squeeze(1)\n",
    "trainset = torch.utils.data.TensorDataset(torch.FloatTensor(X_train),torch.FloatTensor(Y_train))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size, shuffle=True, num_workers=2)\n",
    "testset = torch.utils.data.TensorDataset(torch.FloatTensor(X_test),torch.FloatTensor(Y_test))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"Training model\")\n",
    "for epoch in range(n_epochs):\n",
    "#     print(\"(Outer) Epoch \",epoch,\" of \",n_epochs,\":\")\n",
    "    train(model, epoch, trainloader, criterion, optimizer)\n",
    "print(\"Training Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n",
      "('average error in degree:', 0.9171934685235804)\n",
      "('average error in degree:', 1.1211560138381353)\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving model\")\n",
    "torch.save(model.state_dict(),'model_regression.pth')\n",
    "\n",
    "m,_,_ = make_model(inp_dimn=X_train.shape[1], out_dimn=1, net=net, n_hidden=n_hidden) \n",
    "state = torch.load('model_regression.pth')\n",
    "m.load_state_dict(state)\n",
    "\n",
    "acc = get_accuracy(model, X_train, Y_train, net)\n",
    "acc = get_accuracy(model, X_test, Y_test, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## let us visualize the predicted and true orientation vs distance plots\n",
    "preds = predict(model, X_test)\n",
    "ref = [10,0]\n",
    "d = np.square(data[train_max:,0]-ref[0])+np.square(data[train_max:,1]-ref[1])\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.scatter(d,preds,marker='.')\n",
    "plt.xlabel(\"squared distance from (10,0)\")\n",
    "plt.ylabel(\"predicted_orientation\")\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(212)\n",
    "orientation_vs_dist(data[:,0], data[:,1], data[:,17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
